{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for managing the master beacon annotatons\n",
    "\n",
    "See the README.md for setup.\n",
    "\n",
    "NB: This notebook is unreliable unless cells are run one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import datetime\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "# Depends on: pip install --upgrade google-cloud-bigquery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Some matplotlib features are version dependent.\n",
    "assert(matplotlib.__version__ >= '2.1.2')\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global flags\n",
    "\n",
    "Invoke cell individually or reorder them to change defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EndDate = '2018-05-13'\n",
    "project = 'mlab-sandbox'\n",
    "dataset_id = 'mattmathis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip slow/expensive queries when incrementally rerunning cells\n",
    "DoQueries=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force queries\n",
    "DoQueries=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive figures that pan and zoom\n",
    "interactive = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable figures to load inline in the browser and saved (github etc).\n",
    "interactive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default inline\n"
     ]
    }
   ],
   "source": [
    "def setupmatplotlib(force=None):\n",
    "    global interactive\n",
    "    if force == 'inline':\n",
    "        %matplotlib inline\n",
    "        return\n",
    "    elif force == 'interactive':\n",
    "        %matplotlib\n",
    "        return\n",
    "    elif force is not None:\n",
    "        print 'Unknown option, using default'\n",
    "    if interactive:\n",
    "        print 'default interactive'\n",
    "        %matplotlib\n",
    "        return\n",
    "    else:\n",
    "        print 'default inline'\n",
    "        %matplotlib inline\n",
    "        return\n",
    "setupmatplotlib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# BigQuery interface\n",
    "def expand_query(query, **kwargs):\n",
    "    \"\"\"expand_query: expands nested {parameter} substitutions.\n",
    "    Stashes forensic output in globals.\n",
    "    \"\"\"\n",
    "    global DebugQuery # For pasting into BQ, after the fact\n",
    "    global NumberedQuery # For grocking BQ error line numbers.\n",
    "    global DefaultArgs # To ignore some \n",
    "\n",
    "    # Only allow argument substitution 4 levels deep, because\n",
    "    # accidental infinite recursion risks crashing the notebook.\n",
    "    args = DefaultArgs.copy()\n",
    "    args.update(kwargs)\n",
    "    query=query.format(**args)\n",
    "    query=query.format(**args)\n",
    "    query=query.format(**args)\n",
    "    query=query.format(**args)\n",
    "    if '{' in query:\n",
    "        raise \"Unexpanded substitutions\"\n",
    "    \n",
    "    # Leave crumbs incase we need a postmortem\n",
    "    DebugQuery = query\n",
    "    NumberedQuery = \"\"\n",
    "    for i, l in enumerate(query.split('\\n')):\n",
    "          NumberedQuery += \"%3d %s\\n\"%(i, l)\n",
    "\n",
    "    return query\n",
    "\n",
    "def run_query(query, project=project, otherindex=None, timeindex='partition_date', **kwargs):\n",
    "    \"\"\" run_query\n",
    "        Accepts nested {parameter} substitutions.\n",
    "        \n",
    "        Stashes forensic output in globals.\n",
    "    \"\"\"\n",
    "    global NumberedQuery\n",
    "    query=expand_query(query,  **kwargs)\n",
    "\n",
    "    # do the work\n",
    "    client = bigquery.Client(project=project)\n",
    "    job = client.query(query)  # All errors are delayed\n",
    "\n",
    "    # Marshal the results, catching async errors\n",
    "    try:\n",
    "        results = collections.defaultdict(list)\n",
    "        for row in job.result(timeout=300):\n",
    "            for key in row.keys():\n",
    "                results[key].append(row.get(key)) \n",
    "    except:\n",
    "        print NumberedQuery\n",
    "        raise\n",
    "\n",
    "    if otherindex:\n",
    "        return pd.DataFrame(results, index=results[timeined])\n",
    "    if timeindex:\n",
    "        return pd.DataFrame(results, index=pd.DatetimeIndex(results[timeindex]))\n",
    "    # set timeindex=None to force a raw DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def write_query_table(query, otable,\n",
    "                      project=project,\n",
    "                      dataset_id=dataset_id,\n",
    "                      **kwargs):\n",
    "    \"\"\" write_query_table\n",
    "        Accepts nested {parameter} substitutions.\n",
    "        \n",
    "        Stashes forensic output in globals.\n",
    "    \"\"\"\n",
    "    global NumberedQuery\n",
    "    query=expand_query(query,  **kwargs)\n",
    "\n",
    "    # do the work\n",
    "    client = bigquery.Client(project=project)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    table_ref = client.dataset(dataset_id).table(otable)\n",
    "    job_config.destination = table_ref\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job = client.query(query, location='US', job_config=job_config)\n",
    "\n",
    "    # Marshal the results, catching async errors\n",
    "    try:\n",
    "        res = job.result()  # Get the first row to make sure it starts\n",
    "        while not job.done():\n",
    "            print 'tick'\n",
    "            time.sleep(5)\n",
    "        assert job.state == 'DONE'\n",
    "    except:\n",
    "        print \"Query Errored\"\n",
    "        print NumberedQuery\n",
    "        raise\n",
    "    print \"Query completed\"\n",
    "    return\n",
    "\n",
    "# test code\n",
    "if False:\n",
    "    testQ=\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{dateset_id}.master_annotations`\n",
    "    \"\"\"\n",
    "    write_query_table(testQ, otable='test_results2')\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quieries for creating table master_annotations, which is the core of the paper\n",
    "\n",
    "# Query the list of beacons\n",
    "BeaconQ =\"\"\"\n",
    "SELECT\n",
    "  {beacon_fields}\n",
    "FROM\n",
    "  `{dataset_id}.master_annotations`\n",
    "WHERE\n",
    "  clientIP NOT IN (\n",
    "    '45.56.98.222',\n",
    "    '64.9.225.99',\n",
    "    '64.9.225.190' ) # exclude eb, etc\n",
    "    {beacon_where}\n",
    "\"\"\"\n",
    "\n",
    "# A useful diagnostic query\n",
    "CountQ=\"\"\"\n",
    "SELECT\n",
    "    count(*) as CNT\n",
    "from (\n",
    "{beacons}\n",
    "     )\n",
    "\"\"\"\n",
    "    \n",
    "# New decombosible Master Beacon query\n",
    "RawStats=\"\"\"\n",
    "      web100_log_entry.connection_spec.remote_ip AS clientIP,\n",
    "      IFNULL(substr(connection_spec.server_hostname, -25, 3), \"UNK\") AS server_metro,\n",
    "      COUNT(*) AS num_tests,\n",
    "      MIN( web100_log_entry.log_time ) AS first_time,\n",
    "      MAX( web100_log_entry.log_time ) AS last_time,\n",
    "      COUNTIF(connection_spec.data_direction = 0) AS num_upload,\n",
    "      COUNTIF(connection_spec.data_direction = 1) AS num_download,\n",
    "      COUNTIF(substr(connection_spec.client_application,1,3) = \"cli\") AS num_cli,\n",
    "      COUNTIF(REGEXP_CONTAINS(connection_spec.client_application, \"confine\")) AS num_confine,\n",
    "      COUNTIF(connection_spec.client_browser = \"- (web100clt)\") AS num_web100clt,\n",
    "      COUNTIF(substr(connection_spec.client_browser,1,12) = \"BTWebClient/\") AS num_BTWebClient,\n",
    "      COUNTIF(substr(connection_spec.client_browser,1,7) = \"Chrome/\") AS num_Chrome,\n",
    "      COUNTIF(substr(connection_spec.client_browser,1,8) = \"Firefox/\") AS num_Firefox,\n",
    "      COUNTIF(substr(connection_spec.client_browser,1,3) = \"IE/\") AS num_IE,\n",
    "      SUM(IF(connection_spec.data_direction = 0, web100_log_entry.snap.HCDataOctetsIn, 0)) AS upload_bytes,\n",
    "      SUM(IF(connection_spec.data_direction = 1, web100_log_entry.snap.HCDataOctetsOut, 0)) AS download_bytes,\n",
    "      SUM(IF(connection_spec.data_direction = 0, web100_log_entry.snap.duration, 0)) AS upload_duration,\n",
    "      SUM(IF(connection_spec.data_direction = 1, web100_log_entry.snap.duration, 0)) AS download_duration\n",
    "\"\"\"\n",
    "\n",
    "MetroAgg=\"\"\"\n",
    "    clientIP,\n",
    "    ARRAY_AGG(CONCAT(\"'\",server_metro,\"':\",CAST(num_tests AS STRING)) ORDER BY num_tests DESC) as metro_details,\n",
    "    COUNT(DISTINCT server_metro) AS series_num_metros,\n",
    "    MIN(first_time) AS series_start,\n",
    "    MAX(last_time) AS series_end,\n",
    "    SUM(num_tests) AS series_count,\n",
    "    SUM(num_upload) AS series_uploads,\n",
    "    SUM(num_download) AS series_downloads,\n",
    "    SUM(num_cli) AS series_num_cli,\n",
    "    SUM(num_confine) AS series_num_confine,\n",
    "    SUM(num_web100clt) AS series_num_web100clt,\n",
    "    SUM(num_BTWebClient) AS series_num_BTWebClient,\n",
    "    SUM(num_Chrome) AS series_num_Chrome,\n",
    "    SUM(num_Firefox) AS series_num_Firefox,\n",
    "    SUM(num_IE) AS series_num_IE,\n",
    "    SUM(upload_bytes) AS series_upload_bytes,\n",
    "    SUM(download_bytes) AS series_download_bytes,\n",
    "    SUM(upload_duration) AS series_upload_duration,\n",
    "    SUM(download_duration) AS series_download_duration\n",
    "\"\"\"\n",
    "\n",
    "SummaryStats=\"\"\"\n",
    "  * EXCEPT ( metro_details ),\n",
    "  SAFE_DIVIDE(series_upload_bytes, series_upload_duration)*125.0 AS series_average_upload, # kbps\n",
    "  SAFE_DIVIDE(series_download_bytes, series_download_duration)*125.0 AS series_average_download, # kbps\n",
    "  TIMESTAMP_SECONDS(series_start) AS series_start_asc,\n",
    "  TIMESTAMP_SECONDS(series_end) AS series_end_asc,\n",
    "  ( series_end - series_start ) / 86400.0 AS series_elapsed_days,\n",
    "  ( series_end - series_start ) / (series_count - 1) / 3600.0 AS series_interval_hours,\n",
    "  ARRAY_TO_STRING(metro_details, \" \") AS series_metro_details\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MasterQ=\"\"\"\n",
    "#standardSQL\n",
    "#\n",
    "# Gather a master table summarizing all clients that run repeated tests\n",
    "#\n",
    "# Patch (sed etc) to replace NUM_TESTS\n",
    "SELECT\n",
    "    {SummaryStats}\n",
    "FROM (\n",
    "  SELECT\n",
    "    {MetroAgg}\n",
    "  FROM (\n",
    "    SELECT\n",
    "      {RawStats}\n",
    "    FROM\n",
    "      `measurement-lab.release.ndt_all`\n",
    "    WHERE\n",
    "        partition_date <= '{enddate}'\n",
    "    GROUP BY\n",
    "      clientIP,\n",
    "      server_metro)\n",
    "  GROUP BY\n",
    "    clientIP )\n",
    "  INNER JOIN (\n",
    "      SELECT\n",
    "          ClientIP, dailymax\n",
    "      FROM\n",
    "          `{dataset_id}.clients_test_count`\n",
    "      )\n",
    "  USING ( clientIP )\n",
    "WHERE\n",
    "#  series_count > NUM_TESTS OR\n",
    "  ( series_end - series_start ) / 86400.0 > {MIN_DURATION}\n",
    "    AND ( series_end - series_start ) / (series_count - 1) < {MAX_INTERVAL}*24*60*60 # 2 weeks + fudge\n",
    "    AND dailymax < {DAILY_MAX}\n",
    "ORDER BY series_num_metros DESC\n",
    "\"\"\"\n",
    "\n",
    "global EndDate # prevent irrelevant changes\n",
    "\n",
    "# Default values for optional parameters\n",
    "DefaultArgs = {\n",
    "    'RawStats':RawStats,\n",
    "    'MetroAgg':MetroAgg,\n",
    "    'SummaryStats':SummaryStats,\n",
    "    'enddate':EndDate,   # TODO, must enforce\n",
    "# Common fields used for parsing the master beacons data\n",
    "    'beacons':BeaconQ,\n",
    "    'beacon_fields':'ClientIP',\n",
    "    'beacon_where':'',\n",
    "    'dataset_id':dataset_id \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find all clients that ran too many tests per day\n",
    "AbusiveQ=\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM (\n",
    "    SELECT\n",
    "        clientIP,\n",
    "        MAX(dailycnt) AS dailymax\n",
    "    FROM (\n",
    "        SELECT\n",
    "            partition_date,\n",
    "            web100_log_entry.connection_spec.remote_ip AS clientIP,\n",
    "            count (*) as dailycnt\n",
    "        FROM\n",
    "           `measurement-lab.release.ndt_all`   \n",
    "        WHERE\n",
    "            partition_date <= '{enddate}'\n",
    "        GROUP BY\n",
    "            partition_date, clientIP\n",
    "        )\n",
    "    GROUP BY clientIP\n",
    "    )\n",
    "WHERE\n",
    "    dailymax >= {dailylimit}\n",
    "# ORDER BY dailymax DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Test code to look at abusive clients\n",
    "if False:\n",
    "    absusive_clients = run_query(AbusiveQ, timeindex=None, dailylimit = 100)\n",
    "    print len(absusive_clients)\n",
    "    print absusive_clients[0:10]\n",
    "    print absusive_clients[-10:-1]\n",
    "\n",
    "# This was useful for exploring the data\n",
    "if False:\n",
    "    write_query_table(AbusiveQ, otable='abusive_clients', dailylimit = 100)\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query completed\n",
      "CPU times: user 48 ms, sys: 16 ms, total: 64 ms\n",
      "Wall time: 1min 2s\n",
      "Query completed\n",
      "CPU times: user 60 ms, sys: 16 ms, total: 76 ms\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "# DO the heavy lifting\n",
    "# This produces the annotated master beacon table used in the MLab beacons paper\n",
    "# Once the output table has been created, this query only needs to be rerun if\n",
    "# somthing above changes.\n",
    "\n",
    "if True:\n",
    "    %time write_query_table(AbusiveQ, otable='clients_test_count', dailylimit = 0)    \n",
    "    query = expand_query(MasterQ, MIN_DURATION=365, MAX_INTERVAL=20, DAILY_MAX=100)\n",
    "    %time write_query_table(query, otable='master_annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
